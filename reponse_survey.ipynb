{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\telat\\AppData\\Local\\Temp\\ipykernel_42644\\251764282.py:57: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned_survey_data[columns_to_replace] = cleaned_survey_data[columns_to_replace].replace(value_mapping)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 25)\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = './Datasets/Personality Survey Data.xlsx'\n",
    "\n",
    "# Load the 'Personal Characteristics Survey' sheet into a pandas DataFrame\n",
    "survey_data = pd.read_excel(file_path, sheet_name='Personal Characteristics Survey')\n",
    "\n",
    "# Remove the first few rows to clean up the headers\n",
    "cleaned_survey_data = pd.read_excel(file_path, sheet_name='Personal Characteristics Survey', header=1)\n",
    "\n",
    "\n",
    "# Drop any rows where all columns are NaN\n",
    "cleaned_survey_data.dropna(how='all', inplace=True)\n",
    "\n",
    "# Reset index after dropping NaN rows\n",
    "cleaned_survey_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Remove columns where all values are NaN\n",
    "cleaned_survey_data = cleaned_survey_data.dropna(axis=1, how='all')\n",
    "\n",
    "\n",
    "\n",
    "# Reassign the first row to be the column headers\n",
    "cleaned_survey_data.columns = cleaned_survey_data.iloc[0]\n",
    "\n",
    "# Drop the first row now that it's set as column headers\n",
    "cleaned_survey_data = cleaned_survey_data.drop(0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "retailer_index = cleaned_survey_data[cleaned_survey_data['PLAYER NAME'] == 'retailer22'].index.min()\n",
    "\n",
    "# If the index is found, delete all rows after the first instance of \"retailer22\"\n",
    "if pd.notna(retailer_index):\n",
    "    cleaned_survey_data = cleaned_survey_data.iloc[:retailer_index + 1]\n",
    "\n",
    "columns_to_drop = ['S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10']\n",
    "\n",
    "# Drop those columns if they exist in the dataframe\n",
    "cleaned_survey_data = cleaned_survey_data.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "cleaned_survey_data = cleaned_survey_data.rename(columns={cleaned_survey_data.columns[3]: 'Self Esteem Average'})\n",
    "\n",
    "value_mapping = {\n",
    "    'Completely Disagree': 1,\n",
    "    'Somewhat Disagree': 2,\n",
    "    'Moderately Disagree': 3,\n",
    "    'Not Sure': 4,\n",
    "    'Somewhat Agree': 5,\n",
    "    'Moderately Agree': 6,\n",
    "    'Completely Agree': 7\n",
    "}\n",
    "\n",
    "# Replace the values in columns R1 to R5 using the mapping\n",
    "columns_to_replace = ['R1', 'R2', 'R3', 'R4', 'R5']\n",
    "cleaned_survey_data[columns_to_replace] = cleaned_survey_data[columns_to_replace].replace(value_mapping)\n",
    "\n",
    "\n",
    "cleaned_survey_data[\"Regret Scale Average\"] = (cleaned_survey_data['R1'].astype(int) + cleaned_survey_data['R2'].astype(int) + cleaned_survey_data['R3'].astype(int) + cleaned_survey_data['R4'].astype(int) + cleaned_survey_data['R5'].astype(int)) / 5\n",
    "\n",
    "\n",
    "cleaned_survey_data = cleaned_survey_data.dropna(axis=1, how='all')\n",
    "\n",
    "cleaned_survey_data = cleaned_survey_data.drop(columns=['R1', 'R2', 'R3', 'R4', 'R5'], errors='ignore')\n",
    "\n",
    "# Move \"Regret Scale Average\" next to \"Self Esteem Average\"\n",
    "# First, remove the column \"Regret Scale Average\" and store it temporarily\n",
    "regret_scale_average = cleaned_survey_data.pop('Regret Scale Average')\n",
    "\n",
    "# Insert the \"Regret Scale Average\" column right after \"Self Esteem Average\"\n",
    "self_esteem_index = cleaned_survey_data.columns.get_loc('Self Esteem Average')\n",
    "cleaned_survey_data.insert(self_esteem_index + 1, 'Regret Scale Average', regret_scale_average)\n",
    "\n",
    "rl_value_mapping = {\n",
    "    'RL1': 2,\n",
    "    'RL2': 1.5,\n",
    "    'RL3': 1,\n",
    "    'RL4': 0.5,\n",
    "    'RL5': 0,\n",
    "    'RL6': -0.5,\n",
    "    'RL7': -1\n",
    "}\n",
    "\n",
    "# Initialize a new column \"Tossing Coin Risk Averse Coefficient\"\n",
    "cleaned_survey_data['Tossing Coin Risk Averse Coefficient'] = None\n",
    "\n",
    "# Assign values based on the first occurrence of RL value equal to 0\n",
    "\n",
    "for index, row in cleaned_survey_data.iterrows():\n",
    "    for rl_column in ['RL 1', 'RL 2', 'RL 3', 'RL 4', 'RL 5', 'RL 6', 'RL 7']:\n",
    "        if row[rl_column] == 0:  # Check if the RL value is 0\n",
    "            cleaned_survey_data.loc[index, 'Tossing Coin Risk Averse Coefficient'] = rl_value_mapping.get(rl_column.replace(' ', ''))\n",
    "            break\n",
    "\n",
    "def calculate_risk_value(row):\n",
    "    # Calculate Risk1 value\n",
    "    risk1_value = 1 if row['Risk1'] < 100 else 0 if row['Risk1'] == 100 else -1\n",
    "\n",
    "    # Calculate Risk 2 value\n",
    "    risk2_value = 0.5 if row['Risk 2'] == '100 TL' else -0.5 if row['Risk 2'] == 'Ticket' else 0\n",
    "\n",
    "    # Calculate Risk 3 value\n",
    "    risk3_value = 1 if row['Risk 3'] < 100 else 0 if row['Risk 3'] == 100 else -1\n",
    "\n",
    "    # Sum Risk values\n",
    "    total_risk = risk1_value + risk2_value + risk3_value\n",
    "    return total_risk\n",
    "\n",
    "# Apply the risk calculation\n",
    "cleaned_survey_data['Risk Sum'] = cleaned_survey_data.apply(calculate_risk_value, axis=1)\n",
    "\n",
    "# Calculate the \"Risk Averse Coefficient\" by summing the Risk Sum and Tossing Coin Risk Averse Coefficient\n",
    "cleaned_survey_data['Risk Averse Coefficient'] = cleaned_survey_data['Risk Sum'] + pd.to_numeric(cleaned_survey_data['Tossing Coin Risk Averse Coefficient'], errors='coerce')\n",
    "\n",
    "# Drop the unnecessary columns: Risk1, Risk 2, Risk 3, RL1 through RL7, and Tossing Coin Risk Averse Coefficient\n",
    "columns_to_drop = ['Risk1', 'Risk 2', 'Risk 3', 'RL 1', 'RL 2', 'RL 3', 'RL 4', 'RL 5', 'RL 6', 'RL 7', 'Tossing Coin Risk Averse Coefficient']\n",
    "cleaned_survey_data = cleaned_survey_data.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "risk_averse_coefficient = cleaned_survey_data.pop('Risk Averse Coefficient')\n",
    "\n",
    "# Insert the \"Risk Averse Coefficient\" column right after \"Regret Scale Average\"\n",
    "regret_scale_index = cleaned_survey_data.columns.get_loc('Regret Scale Average')\n",
    "cleaned_survey_data.insert(regret_scale_index + 1, 'Risk Averse Coefficient', risk_averse_coefficient)\n",
    "\n",
    "points = {\n",
    "    'F3': 0,  # 30 TL offer acceptance\n",
    "    'F4': 1,  # 40 TL offer acceptance\n",
    "    'F5': 2,  # 50 TL offer acceptance\n",
    "    'F6': 3,  # 10 TL offer acceptance\n",
    "    'F7': 4  # 20 TL offer acceptance\n",
    "}\n",
    "\n",
    "# Calculate Fairness for Q1 and Q2 based on F6 and F7\n",
    "cleaned_survey_data['Fairness_Q1'] = (100 - cleaned_survey_data['F1']) / 100\n",
    "cleaned_survey_data['Fairness_Q2'] = (100 - cleaned_survey_data['F2']) / 100\n",
    "\n",
    "# Calculate Fairness_Q3 based on weighted points from F1 to F5\n",
    "cleaned_survey_data['Fairness_Q3'] = cleaned_survey_data[[col for col in points.keys()]].multiply(points.values(), axis=1).sum(axis=1) / 10\n",
    "\n",
    "# Calculate the overall Fairness Index\n",
    "cleaned_survey_data['Fairness Index'] = (cleaned_survey_data['Fairness_Q1'] + cleaned_survey_data['Fairness_Q2'] + cleaned_survey_data['Fairness_Q3']) / 3\n",
    "\n",
    "# Drop the F1 to F7 columns\n",
    "cleaned_survey_data = cleaned_survey_data.drop(columns=['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7'], errors='ignore')\n",
    "\n",
    "cleaned_survey_data = cleaned_survey_data.drop([\"Fairness_Q1\", \"Fairness_Q2\", \"Fairness_Q3\"], axis=1)\n",
    "\n",
    "cleaned_survey_data = cleaned_survey_data.loc[:, cleaned_survey_data.columns.notna()]\n",
    "\n",
    "# Display the cleaned data\n",
    "cleaned_survey_data.head()\n",
    "\n",
    "\n",
    "cleaned_survey_data.to_csv('adjusted_reponse_survey.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F1\n",
       "50     20\n",
       "100    11\n",
       "60      3\n",
       "90      3\n",
       "75      2\n",
       "80      2\n",
       "99      1\n",
       "55      1\n",
       "20      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_survey_data[\"F1\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
