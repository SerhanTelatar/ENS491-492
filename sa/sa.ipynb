{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\telat\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\telat\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\telat\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.5 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.5 MB 493.7 kB/s eta 0:00:23\n",
      "   - -------------------------------------- 0.5/11.5 MB 493.7 kB/s eta 0:00:23\n",
      "   -- ------------------------------------- 0.8/11.5 MB 568.6 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 0.8/11.5 MB 568.6 kB/s eta 0:00:19\n",
      "   --- ------------------------------------ 1.0/11.5 MB 621.7 kB/s eta 0:00:17\n",
      "   ---- ----------------------------------- 1.3/11.5 MB 671.3 kB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 1.3/11.5 MB 671.3 kB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 1.6/11.5 MB 711.2 kB/s eta 0:00:14\n",
      "   ------ --------------------------------- 1.8/11.5 MB 745.8 kB/s eta 0:00:13\n",
      "   ------- -------------------------------- 2.1/11.5 MB 753.0 kB/s eta 0:00:13\n",
      "   ------- -------------------------------- 2.1/11.5 MB 753.0 kB/s eta 0:00:13\n",
      "   -------- ------------------------------- 2.4/11.5 MB 771.6 kB/s eta 0:00:12\n",
      "   -------- ------------------------------- 2.4/11.5 MB 771.6 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 2.6/11.5 MB 774.3 kB/s eta 0:00:12\n",
      "   --------- ------------------------------ 2.6/11.5 MB 774.3 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 2.9/11.5 MB 749.0 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 2.9/11.5 MB 749.0 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 2.9/11.5 MB 749.0 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 3.1/11.5 MB 696.5 kB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 3.1/11.5 MB 696.5 kB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 3.1/11.5 MB 696.5 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 3.4/11.5 MB 662.3 kB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 3.4/11.5 MB 662.3 kB/s eta 0:00:13\n",
      "   ------------ --------------------------- 3.7/11.5 MB 657.0 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 3.7/11.5 MB 657.0 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 3.7/11.5 MB 657.0 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 3.7/11.5 MB 657.0 kB/s eta 0:00:12\n",
      "   ------------ --------------------------- 3.7/11.5 MB 657.0 kB/s eta 0:00:12\n",
      "   ------------- -------------------------- 3.9/11.5 MB 603.8 kB/s eta 0:00:13\n",
      "   ------------- -------------------------- 3.9/11.5 MB 603.8 kB/s eta 0:00:13\n",
      "   -------------- ------------------------- 4.2/11.5 MB 593.5 kB/s eta 0:00:13\n",
      "   -------------- ------------------------- 4.2/11.5 MB 593.5 kB/s eta 0:00:13\n",
      "   --------------- ------------------------ 4.5/11.5 MB 592.6 kB/s eta 0:00:12\n",
      "   --------------- ------------------------ 4.5/11.5 MB 592.6 kB/s eta 0:00:12\n",
      "   --------------- ------------------------ 4.5/11.5 MB 592.6 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 588.1 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 588.1 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 588.1 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 4.7/11.5 MB 588.1 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 562.4 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 562.4 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 562.4 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 562.4 kB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 562.4 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 5.2/11.5 MB 528.6 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 5.2/11.5 MB 528.6 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 5.2/11.5 MB 528.6 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 5.2/11.5 MB 528.6 kB/s eta 0:00:12\n",
      "   ------------------- -------------------- 5.5/11.5 MB 512.3 kB/s eta 0:00:12\n",
      "   ------------------- -------------------- 5.5/11.5 MB 512.3 kB/s eta 0:00:12\n",
      "   ------------------- -------------------- 5.5/11.5 MB 512.3 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 5.8/11.5 MB 507.0 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 5.8/11.5 MB 507.0 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 5.8/11.5 MB 507.0 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 6.0/11.5 MB 504.3 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 6.0/11.5 MB 504.3 kB/s eta 0:00:11\n",
      "   --------------------- ------------------ 6.3/11.5 MB 506.4 kB/s eta 0:00:11\n",
      "   --------------------- ------------------ 6.3/11.5 MB 506.4 kB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 6.6/11.5 MB 513.6 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 6.6/11.5 MB 513.6 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 6.8/11.5 MB 521.7 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 6.8/11.5 MB 521.7 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 6.8/11.5 MB 521.7 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.1/11.5 MB 516.2 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.1/11.5 MB 516.2 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.1/11.5 MB 516.2 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 7.3/11.5 MB 514.8 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 7.3/11.5 MB 514.8 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 7.3/11.5 MB 514.8 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 7.3/11.5 MB 514.8 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 7.3/11.5 MB 514.8 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 7.3/11.5 MB 514.8 kB/s eta 0:00:09\n",
      "   -------------------------- ------------- 7.6/11.5 MB 486.8 kB/s eta 0:00:09\n",
      "   -------------------------- ------------- 7.6/11.5 MB 486.8 kB/s eta 0:00:09\n",
      "   -------------------------- ------------- 7.6/11.5 MB 486.8 kB/s eta 0:00:09\n",
      "   --------------------------- ------------ 7.9/11.5 MB 487.0 kB/s eta 0:00:08\n",
      "   --------------------------- ------------ 7.9/11.5 MB 487.0 kB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 8.1/11.5 MB 490.1 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 8.1/11.5 MB 490.1 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 8.4/11.5 MB 493.0 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 8.4/11.5 MB 493.0 kB/s eta 0:00:07\n",
      "   ------------------------------ --------- 8.7/11.5 MB 496.7 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 8.7/11.5 MB 496.7 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 8.7/11.5 MB 496.7 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 8.9/11.5 MB 494.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 8.9/11.5 MB 494.8 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 9.2/11.5 MB 497.8 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 9.2/11.5 MB 497.8 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 9.4/11.5 MB 502.3 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 9.4/11.5 MB 502.3 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 9.4/11.5 MB 502.3 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 9.7/11.5 MB 498.7 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 9.7/11.5 MB 498.7 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 9.7/11.5 MB 498.7 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 9.7/11.5 MB 498.7 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 10.0/11.5 MB 491.9 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 10.0/11.5 MB 491.9 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 10.0/11.5 MB 491.9 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 10.0/11.5 MB 491.9 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 10.2/11.5 MB 488.5 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 10.2/11.5 MB 488.5 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 10.5/11.5 MB 489.8 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 10.5/11.5 MB 489.8 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 10.7/11.5 MB 491.6 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 10.7/11.5 MB 491.6 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 11.0/11.5 MB 493.5 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 11.0/11.5 MB 493.5 kB/s eta 0:00:02\n",
      "   ---------------------------------------  11.3/11.5 MB 496.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.3/11.5 MB 496.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 497.6 kB/s eta 0:00:00\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_data = pd.read_csv('../Notebooks/experiment/curr_data.csv')\n",
    "curr_data_bb = pd.read_csv('../Notebooks/experiment/curr_data_bb.csv')\n",
    "curr_data_rs = pd.read_csv('../Notebooks/experiment/curr_data_rs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Period</th>\n",
       "      <th>Player ID</th>\n",
       "      <th>Demand</th>\n",
       "      <th>Wholesale p.</th>\n",
       "      <th>Revenue Share</th>\n",
       "      <th>Player ID.1</th>\n",
       "      <th>Stock</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Leftovers</th>\n",
       "      <th>...</th>\n",
       "      <th>Expected Sales</th>\n",
       "      <th>Expected_Leftovers</th>\n",
       "      <th>Expected Retailer Profit</th>\n",
       "      <th>Expected Mfg Profit</th>\n",
       "      <th>Expected Mfg. Profit Share</th>\n",
       "      <th>Predicted Sales</th>\n",
       "      <th>Predicted Leftovers</th>\n",
       "      <th>Predicted Retailer Profit</th>\n",
       "      <th>Predicted Mfg Profit</th>\n",
       "      <th>Predicted Mfg. Profit Share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Manufacturer 1</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Retailer 1</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>82.20</td>\n",
       "      <td>24.80</td>\n",
       "      <td>305.4</td>\n",
       "      <td>411.0</td>\n",
       "      <td>0.573702</td>\n",
       "      <td>91.04</td>\n",
       "      <td>15.96</td>\n",
       "      <td>316.28</td>\n",
       "      <td>455.20</td>\n",
       "      <td>0.590035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Manufacturer 1</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>Retailer 1</td>\n",
       "      <td>150</td>\n",
       "      <td>144</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>100.50</td>\n",
       "      <td>49.50</td>\n",
       "      <td>402.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>0.468254</td>\n",
       "      <td>100.50</td>\n",
       "      <td>49.50</td>\n",
       "      <td>402.00</td>\n",
       "      <td>354.00</td>\n",
       "      <td>0.468254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Manufacturer 1</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>Retailer 1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.30</td>\n",
       "      <td>17.70</td>\n",
       "      <td>241.50</td>\n",
       "      <td>536.10</td>\n",
       "      <td>0.689429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Manufacturer 1</td>\n",
       "      <td>149</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>Retailer 1</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>87.75</td>\n",
       "      <td>29.25</td>\n",
       "      <td>326.5</td>\n",
       "      <td>426.5</td>\n",
       "      <td>0.566401</td>\n",
       "      <td>94.89</td>\n",
       "      <td>22.11</td>\n",
       "      <td>335.34</td>\n",
       "      <td>452.34</td>\n",
       "      <td>0.574269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Manufacturer 1</td>\n",
       "      <td>135</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Retailer 1</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>92.30</td>\n",
       "      <td>19.70</td>\n",
       "      <td>408.4</td>\n",
       "      <td>369.2</td>\n",
       "      <td>0.474794</td>\n",
       "      <td>93.09</td>\n",
       "      <td>18.91</td>\n",
       "      <td>408.72</td>\n",
       "      <td>372.36</td>\n",
       "      <td>0.476725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Experiment  Period       Player ID  Demand  Wholesale p.  Revenue Share  \\\n",
       "0           1       1  Manufacturer 1     122             3              5   \n",
       "1           1       2  Manufacturer 1     144             0              8   \n",
       "2           1       3  Manufacturer 1      57             2              7   \n",
       "3           1       4  Manufacturer 1     149             2              6   \n",
       "4           1       5  Manufacturer 1     135             3              4   \n",
       "\n",
       "  Player ID.1  Stock  Sales  Leftovers  ...  Expected Sales  \\\n",
       "0  Retailer 1     90     90          0  ...           82.20   \n",
       "1  Retailer 1    150    144          6  ...          100.50   \n",
       "2  Retailer 1      0      0          0  ...            0.00   \n",
       "3  Retailer 1    100    100          0  ...           87.75   \n",
       "4  Retailer 1    110    110          0  ...           92.30   \n",
       "\n",
       "   Expected_Leftovers  Expected Retailer Profit  Expected Mfg Profit  \\\n",
       "0               24.80                     305.4                411.0   \n",
       "1               49.50                     402.0                354.0   \n",
       "2              110.00                       0.0                  0.0   \n",
       "3               29.25                     326.5                426.5   \n",
       "4               19.70                     408.4                369.2   \n",
       "\n",
       "   Expected Mfg. Profit Share  Predicted Sales  Predicted Leftovers  \\\n",
       "0                    0.573702            91.04                15.96   \n",
       "1                    0.468254           100.50                49.50   \n",
       "2                         NaN            92.30                17.70   \n",
       "3                    0.566401            94.89                22.11   \n",
       "4                    0.474794            93.09                18.91   \n",
       "\n",
       "   Predicted Retailer Profit  Predicted Mfg Profit  \\\n",
       "0                     316.28                455.20   \n",
       "1                     402.00                354.00   \n",
       "2                     241.50                536.10   \n",
       "3                     335.34                452.34   \n",
       "4                     408.72                372.36   \n",
       "\n",
       "   Predicted Mfg. Profit Share  \n",
       "0                     0.590035  \n",
       "1                     0.468254  \n",
       "2                     0.689429  \n",
       "3                     0.574269  \n",
       "4                     0.476725  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_data_rs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define categorization thresholds\n",
    "def categorize_behavior(row):\n",
    "    deviation = row[\"Under/Overstock_wrt_Optimal\"]  # Deviation from optimal (expected sales)\n",
    "    if row['Stock'] == 0:\n",
    "        return 'Reject'\n",
    "    elif deviation < -15:\n",
    "        return 'Significant Underorder'\n",
    "    elif -15 <= deviation <= 15:\n",
    "        return 'Optimal'\n",
    "    elif deviation > 15:\n",
    "        return 'Overorder'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Apply categorization to each dataset\n",
    "curr_data['Behavioral Category'] = curr_data.apply(categorize_behavior, axis=1)\n",
    "curr_data_bb['Behavioral Category'] = curr_data_bb.apply(categorize_behavior, axis=1)\n",
    "curr_data_rs['Behavioral Category'] = curr_data_rs.apply(categorize_behavior, axis=1)\n",
    "\n",
    "curr_data.to_csv('../Notebooks/experiment/curr_data.csv', index = False)\n",
    "curr_data_bb.to_csv('../Notebooks/experiment/curr_data_bb.csv', index = False)\n",
    "curr_data_rs.to_csv('../Notebooks/experiment/curr_data_rs.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavioral_data = pd.read_csv('../adjusted_reponse_survey.csv')\n",
    "historical_data_wholesale = pd.read_csv('../Notebooks/experiment/curr_data.csv')\n",
    "historical_data_buyback = pd.read_csv('../Notebooks/experiment/curr_data_bb.csv')\n",
    "historical_data_revenue_sharing = pd.read_csv('../Notebooks/experiment/curr_data_rs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, action_space, personality_traits=None, learning_rate=0.1, discount_factor=0.99,\n",
    "                 epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):\n",
    "        self.action_space = action_space\n",
    "        self.personality_traits = personality_traits or {}\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.q_table = defaultdict(float)\n",
    "        self.estimated_human_traits = {'Risk Averse Coefficient': 0.5}\n",
    "        self.human_actions = []\n",
    "        self.behavior_change_threshold = 2  # Adjust as needed\n",
    "\n",
    "    def _state_to_key(self, state):\n",
    "        trait_values = tuple(self.personality_traits.values())\n",
    "        return tuple(state) + trait_values\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if self.detect_behavior_change():\n",
    "            self.epsilon = min(1.0, self.epsilon * 1.1)  # Increase epsilon\n",
    "        else:\n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)  # Decay epsilon\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.action_space.sample()\n",
    "        else:\n",
    "            state_key = self._state_to_key(state)\n",
    "            q_values = [self.q_table[(state_key, a)] for a in range(self.action_space.n)]\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        state_key = self._state_to_key(state)\n",
    "        next_state_key = self._state_to_key(next_state)\n",
    "        max_next_q = max([self.q_table[(next_state_key, a)] for a in range(self.action_space.n)])\n",
    "        adjusted_reward = self.adjust_reward_based_on_traits(reward)\n",
    "        self.q_table[(state_key, action)] += self.learning_rate * (\n",
    "            adjusted_reward + self.discount_factor * max_next_q - self.q_table[(state_key, action)]\n",
    "        )\n",
    "\n",
    "    def update_estimated_human_traits(self, human_action):\n",
    "        self.human_actions.append(human_action)\n",
    "        if len(self.human_actions) > 10:\n",
    "            recent_actions = self.human_actions[-10:]\n",
    "            action_variance = np.var(recent_actions)\n",
    "            max_variance = (self.action_space.n - 1) ** 2 / 12  # Variance of uniform distribution\n",
    "            self.estimated_human_traits['Risk Averse Coefficient'] = max(0, min(1, 1 - action_variance / max_variance))\n",
    "\n",
    "    def adjust_reward_based_on_traits(self, reward):\n",
    "        risk_aversion = self.estimated_human_traits.get('Risk Averse Coefficient', 0.5)\n",
    "        trait_factor = 1.0 + (0.5 - risk_aversion)\n",
    "        adjusted_reward = reward * trait_factor\n",
    "        return adjusted_reward\n",
    "\n",
    "    def detect_behavior_change(self):\n",
    "        if len(self.human_actions) < 20:\n",
    "            return False\n",
    "        recent_actions = self.human_actions[-10:]\n",
    "        previous_actions = self.human_actions[-20:-10]\n",
    "        recent_mean = np.mean(recent_actions)\n",
    "        previous_mean = np.mean(previous_actions)\n",
    "        if abs(recent_mean - previous_mean) > self.behavior_change_threshold:\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_traits(behavioral_data):\n",
    "    \"\"\"\n",
    "    Extract traits for all manufacturers and retailers from the behavioral data.\n",
    "    Returns dictionaries of traits for manufacturers and retailers indexed by PLAYER NAME.\n",
    "    \"\"\"\n",
    "    manufacturers = behavioral_data[[\n",
    "        'Manufacturer_Self Esteem Average',\n",
    "        'Manufacturer_Regret Scale Average',\n",
    "        'Manufacturer_Risk Averse Coefficient',\n",
    "        'Manufacturer_Fairness Index'\n",
    "    ]].rename(lambda col: col.replace('Manufacturer_', ''), axis=1)\n",
    "\n",
    "    retailers = behavioral_data[[\n",
    "        'Retailer_Self Esteem Average',\n",
    "        'Retailer_Regret Scale Average',\n",
    "        'Retailer_Risk Averse Coefficient',\n",
    "        'Retailer_Fairness Index'\n",
    "    ]].rename(lambda col: col.replace('Retailer_', ''), axis=1)\n",
    "\n",
    "    # Return as dictionaries indexed by PLAYER NAME\n",
    "    manufacturer_traits = manufacturers.to_dict(orient='index')\n",
    "    retailer_traits = retailers.to_dict(orient='index')\n",
    "\n",
    "    return manufacturer_traits, retailer_traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n# Extract traits\\nmanufacturer_traits, retailer_traits = extract_traits(behavioral_data)\\n\\n# Sample traits for inspection\\nsample_manufacturer = list(manufacturer_traits.keys())[0]\\nsample_retailer = list(retailer_traits.keys())[0]\\n\\nsample_manufacturer_traits = manufacturer_traits[sample_manufacturer]\\nsample_retailer_traits = retailer_traits[sample_retailer]\\n\\n'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Extract traits\n",
    "manufacturer_traits, retailer_traits = extract_traits(behavioral_data)\n",
    "\n",
    "# Sample traits for inspection\n",
    "sample_manufacturer = list(manufacturer_traits.keys())[0]\n",
    "sample_retailer = list(retailer_traits.keys())[0]\n",
    "\n",
    "sample_manufacturer_traits = manufacturer_traits[sample_manufacturer]\n",
    "sample_retailer_traits = retailer_traits[sample_retailer]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Initialize the environment\\nenv = SupplyChainEnv(contract_type=\"wholesale\")\\n\\n# Initialize agents with personality traits\\nmanufacturer_agent = QLearningAgent(env.manufacturer_action_space, personality_traits=sample_manufacturer_traits)\\nretailer_agent = QLearningAgent(env.retailer_action_space, personality_traits=sample_retailer_traits)\\n'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Redefine the environment class and test setup\n",
    "import numpy as np\n",
    "from gym import Env, spaces\n",
    "\n",
    "class SupplyChainEnv(Env):\n",
    "    \"\"\"\n",
    "    Custom Environment for the supply chain game.\n",
    "    \"\"\"\n",
    "    def __init__(self, contract_type=\"wholesale\"):\n",
    "        super(SupplyChainEnv, self).__init__()\n",
    "        self.contract_type = contract_type\n",
    "        self.max_stock = 150\n",
    "        self.max_price = 12\n",
    "        self.max_rounds = 40\n",
    "        self.current_round = 0\n",
    "\n",
    "        if self.contract_type == \"wholesale\":\n",
    "            self.manufacturer_action_space = spaces.Discrete(self.max_price + 1)\n",
    "            self.retailer_action_space = spaces.Discrete(self.max_stock + 1)\n",
    "        elif self.contract_type == \"buyback\":\n",
    "            self.manufacturer_action_space = spaces.MultiDiscrete([self.max_price + 1, self.max_price + 1])\n",
    "            self.retailer_action_space = spaces.Discrete(self.max_stock + 1)\n",
    "        elif self.contract_type == \"revenue-sharing\":\n",
    "            self.manufacturer_action_space = spaces.MultiDiscrete([self.max_price + 1, self.max_price + 1])\n",
    "            self.retailer_action_space = spaces.Discrete(self.max_stock + 1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid contract type.\")\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=150, shape=(5,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.demand = np.random.randint(50, 151)\n",
    "        self.state = np.array([self.demand, 0, 0, 0, 0])\n",
    "        self.current_round = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, actions):\n",
    "        manufacturer_action, retailer_action = actions\n",
    "        Q = retailer_action\n",
    "        sales = min(Q, self.demand)\n",
    "        leftovers = Q - sales\n",
    "        c = 5\n",
    "        p = self.max_price\n",
    "\n",
    "        if self.contract_type == \"wholesale\":\n",
    "            # Extract wholesale price\n",
    "            w = manufacturer_action\n",
    "            b = 0\n",
    "            r = 0\n",
    "\n",
    "            # Retailer payoff\n",
    "            retailer_profit = p * sales - w * Q\n",
    "\n",
    "            # Manufacturer payoff\n",
    "            manufacturer_profit = (w - c) * Q\n",
    "\n",
    "        elif self.contract_type == \"buyback\":\n",
    "            # Extract wholesale price and buyback price\n",
    "            w, b = manufacturer_action\n",
    "            r = 0\n",
    "\n",
    "            # Enforce constraint: buyback price must not exceed wholesale price\n",
    "            if b > w:\n",
    "                b = w  # Adjust buyback price to wholesale price\n",
    "\n",
    "            # Retailer payoff\n",
    "            retailer_profit = p * sales - w * Q + b * leftovers\n",
    "\n",
    "            # Manufacturer payoff\n",
    "            manufacturer_profit = (w - c) * Q - b * leftovers\n",
    "\n",
    "        elif self.contract_type == \"revenue-sharing\":\n",
    "            # Extract wholesale price and revenue share\n",
    "            w, r = manufacturer_action\n",
    "            b = 0\n",
    "\n",
    "            # Enforce constraint: revenue share must not exceed (retail price - wholesale price)\n",
    "            max_revenue_share = p - w\n",
    "            if r > max_revenue_share:\n",
    "                r = max_revenue_share\n",
    "\n",
    "            # Retailer payoff\n",
    "            retailer_profit = p * sales - w * Q - r * sales\n",
    "\n",
    "            # Manufacturer payoff\n",
    "            manufacturer_profit = (w - c) * Q + r * sales\n",
    "\n",
    "        self.state = np.array([self.demand, w, b, r, retailer_profit, manufacturer_profit])\n",
    "        self.current_round += 1\n",
    "        self.demand = np.random.randint(50, 151)\n",
    "        done = self.current_round >= self.max_rounds\n",
    "\n",
    "        return self.state, (manufacturer_profit, retailer_profit), done, {}\n",
    "\n",
    "\"\"\"\n",
    "# Initialize the environment\n",
    "env = SupplyChainEnv(contract_type=\"wholesale\")\n",
    "\n",
    "# Initialize agents with personality traits\n",
    "manufacturer_agent = QLearningAgent(env.manufacturer_action_space, personality_traits=sample_manufacturer_traits)\n",
    "retailer_agent = QLearningAgent(env.retailer_action_space, personality_traits=sample_retailer_traits)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavioral_data.columns = behavioral_data.columns.str.strip()\n",
    "historical_data_wholesale.columns = historical_data_wholesale.columns.str.strip()\n",
    "historical_data_buyback.columns = historical_data_buyback.columns.str.strip()\n",
    "historical_data_revenue_sharing.columns = historical_data_revenue_sharing.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data_wholesale.columns = historical_data_wholesale.columns.str.replace(' ', '_')\n",
    "historical_data_buyback.columns = historical_data_buyback.columns.str.replace(' ', '_')\n",
    "historical_data_revenue_sharing.columns = historical_data_revenue_sharing.columns.str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract behavioral data\n",
    "behavioral_data = behavioral_data[['PLAYER NAME', \n",
    "                                   'Manufacturer_Self Esteem Average', \n",
    "                                   'Manufacturer_Regret Scale Average', \n",
    "                                   'Manufacturer_Risk Averse Coefficient', \n",
    "                                   'Manufacturer_Fairness Index', \n",
    "                                   'Retailer_Self Esteem Average', \n",
    "                                   'Retailer_Regret Scale Average', \n",
    "                                   'Retailer_Risk Averse Coefficient', \n",
    "                                   'Retailer_Fairness Index']].set_index('PLAYER NAME')\n",
    "\n",
    "# Function to extract historical data for Q-learning\n",
    "def preprocess_historical_data(df, contract_type):\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        # State variables\n",
    "        state = [\n",
    "            row['Demand'], \n",
    "            row['Wholesale_p.'], \n",
    "            row.get('Buyback_p.', 0),  # Default to 0 if not present\n",
    "            row.get('Revenue_Share', 0)  # Default to 0 if not present\n",
    "        ]\n",
    "        \n",
    "        # Actions\n",
    "        manufacturer_action = (\n",
    "            row['Wholesale_p.'], \n",
    "            row.get('Buyback_p.', row.get('Revenue_Share', 0))\n",
    "        )\n",
    "        retailer_action = row['Stock']\n",
    "        \n",
    "        # Rewards\n",
    "        manufacturer_reward = row['Realized_Mfg_Profit']\n",
    "        retailer_reward = row['Realized_Retailer_Profit']\n",
    "        \n",
    "        # Next state (mocked as same for simplicity, typically inferred)\n",
    "        next_state = state\n",
    "        \n",
    "        data.append((state, manufacturer_action, retailer_action, (manufacturer_reward, retailer_reward), next_state))\n",
    "    return data\n",
    "\n",
    "# Process historical data for each contract type\n",
    "historical_data_wholesale = preprocess_historical_data(historical_data_wholesale, \"wholesale\")\n",
    "historical_data_buyback = preprocess_historical_data(historical_data_buyback, \"buyback\")\n",
    "historical_data_revenue_sharing = preprocess_historical_data(historical_data_revenue_sharing, \"revenue-sharing\")\n",
    "\n",
    "# Combine historical data\n",
    "historical_data = historical_data_wholesale + historical_data_buyback + historical_data_revenue_sharing\n",
    "\n",
    "\n",
    "# Initialize agents with personality traits\n",
    "manufacturer_agent = QLearningAgent(env.manufacturer_action_space, personality_traits={'risk_aversion': 0.7, 'fairness': 0.5})\n",
    "retailer_agent = QLearningAgent(env.retailer_action_space, personality_traits={'risk_aversion': 0.3, 'self_esteem': 0.8})\n",
    "\n",
    "# Pre-train Q-table using historical data\n",
    "for record in historical_data:\n",
    "    state, manufacturer_action, retailer_action, rewards, next_state = record\n",
    "    manufacturer_reward, retailer_reward = rewards\n",
    "\n",
    "    # Update Q-table for manufacturer\n",
    "    manufacturer_agent.q_table[(manufacturer_agent._state_to_key(state), manufacturer_action)] = manufacturer_reward\n",
    "\n",
    "    # Update Q-table for retailer\n",
    "    retailer_agent.q_table[(retailer_agent._state_to_key(state), retailer_action)] = retailer_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract traits\n",
    "manufacturer_traits, retailer_traits = extract_traits(behavioral_data)\n",
    "\n",
    "# Sample traits for inspection\n",
    "sample_manufacturer = list(manufacturer_traits.keys())[0]\n",
    "sample_retailer = list(retailer_traits.keys())[0]\n",
    "\n",
    "sample_manufacturer_traits = manufacturer_traits[sample_manufacturer]\n",
    "sample_retailer_traits = retailer_traits[sample_retailer]\n",
    "\n",
    "# Initialize the environment\n",
    "env = SupplyChainEnv(contract_type=\"wholesale\")\n",
    "\n",
    "# Initialize agents with personality traits\n",
    "manufacturer_agent = QLearningAgent(env.manufacturer_action_space, personality_traits=sample_manufacturer_traits)\n",
    "retailer_agent = QLearningAgent(env.retailer_action_space, personality_traits=sample_retailer_traits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wholesale Data Columns: Index(['Experiment', 'Period', 'Player_ID', 'Demand', 'Wholesale_p.',\n",
      "       'Player_ID.1', 'Stock', 'Sales', 'Leftovers', 'Unmet_Demand',\n",
      "       'Realized_Mfg_Profit', 'Realized_Retailer_Profit',\n",
      "       'Realized_Mfg_Profit_Share', 'Optimal_Stock',\n",
      "       'Under/Overstock_wrt_Optimal', 'Expected_Sales', 'Expected_Leftovers',\n",
      "       'Expected_Retailer_Profit', 'Expected_Mfg_Profit',\n",
      "       'Expected_Mfg._Profit_Share', 'Predicted_Sales', 'Predicted_Leftovers',\n",
      "       'Predicted_Retailer_Profit', 'Predicted_Mfg_Profit',\n",
      "       'Realized_Retailer_Profit', 'Predicted_Mfg._Profit_Share',\n",
      "       'Behavioral_Category'],\n",
      "      dtype='object')\n",
      "Buyback Data Columns: Index(['Experiment', 'Period', 'Player_ID', 'Demand', 'Wholesale_p.',\n",
      "       'Buyback_p.', 'Player_ID.1', 'Stock', 'Sales', 'Leftovers',\n",
      "       'Unmet_Demand', 'Realized_Mfg_Profit', 'Realized_Retailer_Profit',\n",
      "       'Realized_Retailer_Profit', 'Realized_Mfg_Profit_Share',\n",
      "       'Optimal_Stock', 'Under/Overstock_wrt_Optimal', 'Expected_Sales',\n",
      "       'Expected_Leftovers', 'Expected_Retailer_Profit', 'Expected_Mfg_Profit',\n",
      "       'Expected_Mfg._Profit_Share', 'Predicted_Sales', 'Predicted_Leftovers',\n",
      "       'Predicted_Retailer_Profit', 'Predicted_Mfg_Profit',\n",
      "       'Predicted_Mfg._Profit_Share', 'Behavioral_Category'],\n",
      "      dtype='object')\n",
      "Revenue Sharing Data Columns: Index(['Experiment', 'Period', 'Player_ID', 'Demand', 'Wholesale_p.',\n",
      "       'Revenue_Share', 'Player_ID.1', 'Stock', 'Sales', 'Leftovers',\n",
      "       'Unmet_Demand', 'Realized_Retailer_Profit', 'Realized_Mfg_Profit',\n",
      "       'Realized_Retailer_Profit', 'Realized_Mfg_Profit_Share',\n",
      "       'Optimal_Stock', 'Under/Overstock_wrt_Optimal', 'Expected_Sales',\n",
      "       'Expected_Leftovers', 'Expected_Retailer_Profit', 'Expected_Mfg_Profit',\n",
      "       'Expected_Mfg._Profit_Share', 'Predicted_Sales', 'Predicted_Leftovers',\n",
      "       'Predicted_Retailer_Profit', 'Predicted_Mfg_Profit',\n",
      "       'Predicted_Mfg._Profit_Share', 'Behavioral_Category'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot convert the series to <class 'float'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 304\u001b[0m\n\u001b[0;32m    301\u001b[0m historical_data_revenue_sharing\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m historical_data_revenue_sharing\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m# Process historical data\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m historical_data_wholesale \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_historical_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistorical_data_wholesale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwholesale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m historical_data_buyback \u001b[38;5;241m=\u001b[39m preprocess_historical_data(historical_data_buyback, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuyback\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    306\u001b[0m historical_data_revenue_sharing \u001b[38;5;241m=\u001b[39m preprocess_historical_data(historical_data_revenue_sharing, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevenue-sharing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 252\u001b[0m, in \u001b[0;36mpreprocess_historical_data\u001b[1;34m(df, contract_type)\u001b[0m\n\u001b[0;32m    250\u001b[0m buyback_price \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuyback_p.\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuyback_p.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m row \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misnull(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuyback_p.\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    251\u001b[0m revenue_share \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevenue_Share\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevenue_Share\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m row \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misnull(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRevenue_Share\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m--> 252\u001b[0m previous_retailer_profit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRealized_Retailer_Profit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m stock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStock\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    254\u001b[0m realized_mfg_profit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRealized_Mfg_Profit\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\series.py:248\u001b[0m, in \u001b[0;36m_coerce_method.<locals>.wrapper\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    240\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconverter\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on a single element Series is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeprecated and will raise a TypeError in the future. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    246\u001b[0m     )\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m converter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot convert the series to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconverter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot convert the series to <class 'float'>"
     ]
    }
   ],
   "source": [
    "# bu errorlu olmayan, ama yanlış\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from gym import Env, spaces\n",
    "import random\n",
    "\n",
    "# Define the SupplyChainEnv class\n",
    "class SupplyChainEnv(Env):\n",
    "    \"\"\"\n",
    "    Custom Environment for the supply chain game.\n",
    "    \"\"\"\n",
    "    def __init__(self, contract_type=\"wholesale\"):\n",
    "        super(SupplyChainEnv, self).__init__()\n",
    "        self.contract_type = contract_type\n",
    "        self.max_stock = 150\n",
    "        self.max_price = 12\n",
    "        self.max_rounds = 40\n",
    "        self.current_round = 0\n",
    "\n",
    "        if self.contract_type == \"wholesale\":\n",
    "            self.manufacturer_action_space = spaces.Discrete(self.max_price + 1)\n",
    "            self.retailer_action_space = spaces.Discrete(self.max_stock + 1)\n",
    "        elif self.contract_type == \"buyback\":\n",
    "            self.manufacturer_action_space = spaces.MultiDiscrete([self.max_price + 1, self.max_price + 1])\n",
    "            self.retailer_action_space = spaces.Discrete(self.max_stock + 1)\n",
    "        elif self.contract_type == \"revenue-sharing\":\n",
    "            self.manufacturer_action_space = spaces.MultiDiscrete([self.max_price + 1, self.max_price + 1])\n",
    "            self.retailer_action_space = spaces.Discrete(self.max_stock + 1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid contract type.\")\n",
    "\n",
    "        self.observation_space = spaces.Box(low=0, high=150, shape=(5,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.demand = np.random.randint(50, 151)\n",
    "        self.state = np.array([self.demand, 0, 0, 0, 0])  # Initial state\n",
    "        self.current_round = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, actions):\n",
    "        manufacturer_action, retailer_action = actions\n",
    "        Q = retailer_action\n",
    "        sales = min(Q, self.demand)\n",
    "        leftovers = Q - sales\n",
    "        c = 5  # Manufacturer's cost\n",
    "        p = 12  # Retail price\n",
    "\n",
    "        if self.contract_type == \"wholesale\":\n",
    "            # Extract wholesale price\n",
    "            w = manufacturer_action\n",
    "            b = 0\n",
    "            r = 0\n",
    "\n",
    "            # Retailer payoff\n",
    "            retailer_profit = p * sales - w * Q\n",
    "\n",
    "            # Manufacturer payoff\n",
    "            manufacturer_profit = (w - c) * Q\n",
    "\n",
    "        elif self.contract_type == \"buyback\":\n",
    "            # Extract wholesale price and buyback price\n",
    "            w, b = manufacturer_action\n",
    "            r = 0\n",
    "\n",
    "            # Enforce constraint: buyback price must not exceed wholesale price\n",
    "            if b > w:\n",
    "                b = w  # Adjust buyback price to wholesale price\n",
    "\n",
    "            # Retailer payoff\n",
    "            retailer_profit = p * sales - w * Q + b * leftovers\n",
    "\n",
    "            # Manufacturer payoff\n",
    "            manufacturer_profit = (w - c) * Q - b * leftovers\n",
    "\n",
    "        elif self.contract_type == \"revenue-sharing\":\n",
    "            # Extract wholesale price and revenue share\n",
    "            w, r = manufacturer_action\n",
    "            b = 0\n",
    "\n",
    "            # Enforce constraint: revenue share must not exceed (retail price - wholesale price)\n",
    "            max_revenue_share = p - w\n",
    "            if r > max_revenue_share:\n",
    "                r = max_revenue_share\n",
    "\n",
    "            # Retailer payoff\n",
    "            retailer_profit = p * sales - w * Q - r * sales\n",
    "\n",
    "            # Manufacturer payoff\n",
    "            manufacturer_profit = (w - c) * Q + r * sales\n",
    "\n",
    "        # Update the state\n",
    "        self.state = np.array([self.demand, w, b, r, retailer_profit, manufacturer_profit])\n",
    "        self.current_round += 1\n",
    "        self.demand = np.random.randint(50, 151)  # Generate new demand for next round\n",
    "        done = self.current_round >= self.max_rounds\n",
    "\n",
    "        # For simplicity, we return the same state as next_state\n",
    "        next_state = self.state.copy()\n",
    "\n",
    "        return next_state, (manufacturer_profit, retailer_profit), done, {}\n",
    "\n",
    "# Define the QLearningAgent class with Trait-Driven Reward Modulation and dynamic Exploration vs. Exploitation Balance\n",
    "class QLearningAgent:\n",
    "    def __init__(self, action_space, personality_traits=None, learning_rate=0.1, discount_factor=0.99,\n",
    "                 epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):\n",
    "        self.action_space = action_space\n",
    "        self.personality_traits = personality_traits or {}\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_epsilon = epsilon  # Store initial epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.q_table = defaultdict(float)\n",
    "        self.estimated_human_traits = {'Risk Averse Coefficient': 0.5}\n",
    "        self.human_actions = []\n",
    "        self.behavior_change_threshold = 2  # Adjust as needed\n",
    "\n",
    "    def _state_to_key(self, state):\n",
    "        trait_values = tuple(self.personality_traits.values())\n",
    "        return tuple(state) + trait_values\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if self.detect_behavior_change():\n",
    "            self.epsilon = min(1.0, self.epsilon * 1.1)  # Increase epsilon to promote exploration\n",
    "        else:\n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)  # Decay epsilon\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Explore: choose a random action\n",
    "            if isinstance(self.action_space, spaces.Discrete):\n",
    "                return self.action_space.sample()\n",
    "            elif isinstance(self.action_space, spaces.MultiDiscrete):\n",
    "                return tuple(self.action_space.sample())\n",
    "        else:\n",
    "            # Exploit: choose the action with the highest Q-value\n",
    "            state_key = self._state_to_key(state)\n",
    "            possible_actions = self.get_possible_actions()\n",
    "            q_values = [self.q_table[(state_key, a)] for a in possible_actions]\n",
    "            max_q = max(q_values)\n",
    "            max_actions = [a for a, q in zip(possible_actions, q_values) if q == max_q]\n",
    "            chosen_action = random.choice(max_actions)\n",
    "            return chosen_action\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        state_key = self._state_to_key(state)\n",
    "        next_state_key = self._state_to_key(next_state)\n",
    "\n",
    "        # Get possible actions for next state\n",
    "        possible_actions = self.get_possible_actions()\n",
    "        max_next_q = max([self.q_table[(next_state_key, a)] for a in possible_actions])\n",
    "\n",
    "        # Adjust the reward based on estimated human traits\n",
    "        adjusted_reward = self.adjust_reward_based_on_traits(reward)\n",
    "\n",
    "        # Update Q-value using the adjusted reward\n",
    "        self.q_table[(state_key, action)] += self.learning_rate * (\n",
    "            adjusted_reward + self.discount_factor * max_next_q - self.q_table[(state_key, action)]\n",
    "        )\n",
    "\n",
    "    def update_estimated_human_traits(self, human_action):\n",
    "        self.human_actions.append(human_action)\n",
    "        if len(self.human_actions) > 10:\n",
    "            recent_actions = self.human_actions[-10:]\n",
    "            action_variance = np.var(recent_actions)\n",
    "            max_variance = (self.action_space.n - 1) ** 2 / 12  # Variance of uniform distribution\n",
    "            self.estimated_human_traits['Risk Averse Coefficient'] = max(0, min(1, 1 - action_variance / max_variance))\n",
    "\n",
    "    def adjust_reward_based_on_traits(self, reward):\n",
    "        risk_aversion = self.estimated_human_traits.get('Risk Averse Coefficient', 0.5)\n",
    "        trait_factor = 1.0 + (0.5 - risk_aversion)  # Adjust factor between 0.5 and 1.5\n",
    "        adjusted_reward = reward * trait_factor\n",
    "        return adjusted_reward\n",
    "\n",
    "    def detect_behavior_change(self):\n",
    "        if len(self.human_actions) < 20:\n",
    "            return False\n",
    "        recent_actions = self.human_actions[-10:]\n",
    "        previous_actions = self.human_actions[-20:-10]\n",
    "        recent_mean = np.mean(recent_actions)\n",
    "        previous_mean = np.mean(previous_actions)\n",
    "        if abs(recent_mean - previous_mean) > self.behavior_change_threshold:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_possible_actions(self):\n",
    "        if isinstance(self.action_space, spaces.Discrete):\n",
    "            return range(self.action_space.n)\n",
    "        elif isinstance(self.action_space, spaces.MultiDiscrete):\n",
    "            ranges = [range(n) for n in self.action_space.nvec]\n",
    "            possible_actions = list(itertools.product(*ranges))\n",
    "            return possible_actions\n",
    "\n",
    "# Import itertools for possible actions\n",
    "import itertools\n",
    "\n",
    "# Function to extract traits for all manufacturers and retailers from the behavioral data\n",
    "def extract_traits(behavioral_data):\n",
    "    # Extract manufacturer traits\n",
    "    manufacturer_traits = behavioral_data[[\n",
    "        'Manufacturer_Self Esteem Average',\n",
    "        'Manufacturer_Regret Scale Average',\n",
    "        'Manufacturer_Risk Averse Coefficient',\n",
    "        'Manufacturer_Fairness Index'\n",
    "    ]].rename(columns=lambda col: col.replace('Manufacturer_', '')).to_dict(orient='index')\n",
    "\n",
    "    # Extract retailer traits\n",
    "    retailer_traits = behavioral_data[[\n",
    "        'Retailer_Self Esteem Average',\n",
    "        'Retailer_Regret Scale Average',\n",
    "        'Retailer_Risk Averse Coefficient',\n",
    "        'Retailer_Fairness Index'\n",
    "    ]].rename(columns=lambda col: col.replace('Retailer_', '')).to_dict(orient='index')\n",
    "\n",
    "    return manufacturer_traits, retailer_traits\n",
    "\n",
    "# Load behavioral data\n",
    "behavioral_data = pd.read_csv('../adjusted_reponse_survey.csv')\n",
    "behavioral_data.columns = behavioral_data.columns.str.strip()\n",
    "\n",
    "# Extract traits\n",
    "manufacturer_traits_dict, retailer_traits_dict = extract_traits(behavioral_data.set_index('PLAYER NAME'))\n",
    "\n",
    "# For simplicity, select a sample manufacturer and retailer\n",
    "sample_manufacturer = list(manufacturer_traits_dict.keys())[0]\n",
    "sample_retailer = list(retailer_traits_dict.keys())[0]\n",
    "\n",
    "sample_manufacturer_traits = manufacturer_traits_dict[sample_manufacturer]\n",
    "sample_retailer_traits = retailer_traits_dict[sample_retailer]\n",
    "\n",
    "# Load and preprocess historical data\n",
    "def preprocess_historical_data(df, contract_type):\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        # State variables\n",
    "        state = [\n",
    "            row['Demand'],\n",
    "            row['Wholesale_p.'],\n",
    "            row.get('Buyback_p.', 0),  # Default to 0 if not present\n",
    "            row.get('Revenue_Share', 0)  # Default to 0 if not present\n",
    "        ]\n",
    "\n",
    "        # Actions\n",
    "        if contract_type == \"wholesale\":\n",
    "            manufacturer_action = row['Wholesale_p.']\n",
    "        else:\n",
    "            manufacturer_action = (row['Wholesale_p.'], row.get('Buyback_p.', row.get('Revenue_Share', 0)))\n",
    "\n",
    "        retailer_action = row['Stock']\n",
    "\n",
    "        # Rewards\n",
    "        manufacturer_reward = row['Realized_Mfg_Profit']\n",
    "        retailer_reward = row['Realized_Retailer_Profit']\n",
    "\n",
    "        # Next state (for simplicity, we use the same state)\n",
    "        next_state = state\n",
    "\n",
    "        data.append((state, manufacturer_action, retailer_action, (manufacturer_reward, retailer_reward), next_state))\n",
    "    return data\n",
    "\n",
    "# Load historical data\n",
    "historical_data_wholesale = pd.read_csv('../Notebooks/experiment/curr_data.csv')\n",
    "historical_data_buyback = pd.read_csv('../Notebooks/experiment/curr_data_bb.csv')\n",
    "historical_data_revenue_sharing = pd.read_csv('../Notebooks/experiment/curr_data_rs.csv')\n",
    "\n",
    "# Clean column names\n",
    "historical_data_wholesale.columns = historical_data_wholesale.columns.str.strip().str.replace(' ', '_')\n",
    "historical_data_buyback.columns = historical_data_buyback.columns.str.strip().str.replace(' ', '_')\n",
    "historical_data_revenue_sharing.columns = historical_data_revenue_sharing.columns.str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Process historical data\n",
    "historical_data_wholesale = preprocess_historical_data(historical_data_wholesale, \"wholesale\")\n",
    "historical_data_buyback = preprocess_historical_data(historical_data_buyback, \"buyback\")\n",
    "historical_data_revenue_sharing = preprocess_historical_data(historical_data_revenue_sharing, \"revenue-sharing\")\n",
    "\n",
    "# Combine historical data\n",
    "historical_data = historical_data_wholesale + historical_data_buyback + historical_data_revenue_sharing\n",
    "\n",
    "# Initialize the environment\n",
    "env = SupplyChainEnv(contract_type=\"wholesale\")  # You can change the contract type as needed\n",
    "\n",
    "# Initialize agents with personality traits\n",
    "manufacturer_agent = QLearningAgent(env.manufacturer_action_space, personality_traits=sample_manufacturer_traits)\n",
    "retailer_agent = QLearningAgent(env.retailer_action_space, personality_traits=sample_retailer_traits)\n",
    "\n",
    "# Pre-train the agents using historical data\n",
    "for record in historical_data:\n",
    "    state, manufacturer_action, retailer_action, rewards, next_state = record\n",
    "    manufacturer_reward, retailer_reward = rewards\n",
    "\n",
    "    # Update Q-table for manufacturer\n",
    "    state_key = manufacturer_agent._state_to_key(state)\n",
    "    manufacturer_agent.q_table[(state_key, manufacturer_action)] = manufacturer_reward\n",
    "\n",
    "    # Update Q-table for retailer\n",
    "    state_key = retailer_agent._state_to_key(state)\n",
    "    retailer_agent.q_table[(state_key, retailer_action)] = retailer_reward\n",
    "\n",
    "# Main training loop\n",
    "num_episodes = 100000  # Number of episodes for training\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Agents select actions\n",
    "        manufacturer_action = manufacturer_agent.get_action(state)\n",
    "        retailer_action = retailer_agent.get_action(state)\n",
    "\n",
    "        # Environment step\n",
    "        next_state, rewards, done, info = env.step((manufacturer_action, retailer_action))\n",
    "        manufacturer_reward, retailer_reward = rewards\n",
    "\n",
    "        # Update estimated human traits (assuming retailer is human for this example)\n",
    "        # In this example, both agents are AI, so we can simulate human actions\n",
    "        # For real human interaction, you would get human_action from user input\n",
    "        manufacturer_agent.update_estimated_human_traits(retailer_action)\n",
    "        retailer_agent.update_estimated_human_traits(manufacturer_action)\n",
    "\n",
    "        # Update Q-tables\n",
    "        manufacturer_agent.update_q_table(state, manufacturer_action, manufacturer_reward, next_state)\n",
    "        retailer_agent.update_q_table(state, retailer_action, retailer_reward, next_state)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "    # Optionally, print progress\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed.\")\n",
    "\n",
    "# The agents are now trained and can be used to play the game against a human\n",
    "# For example, to have the AI manufacturer play against a human retailer:\n",
    "\n",
    "# Initialize environment\n",
    "env = SupplyChainEnv(contract_type=\"wholesale\")  # Adjust contract type as needed\n",
    "\n",
    "# Reset environment\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "print(\"Game start. You are the Retailer.\")\n",
    "\n",
    "while not done:\n",
    "    # Manufacturer (AI) selects action\n",
    "    manufacturer_action = manufacturer_agent.get_action(state)\n",
    "\n",
    "    # Human (Retailer) input\n",
    "    print(f\"Demand: {state[0]}, Manufacturer offers wholesale price: {manufacturer_action}\")\n",
    "    retailer_action = int(input(\"Enter your order quantity (0-150): \"))\n",
    "\n",
    "    # Ensure the action is within the valid range\n",
    "    retailer_action = max(0, min(env.retailer_action_space.n - 1, retailer_action))\n",
    "\n",
    "    # Environment step\n",
    "    next_state, rewards, done, info = env.step((manufacturer_action, retailer_action))\n",
    "    manufacturer_reward, retailer_reward = rewards\n",
    "\n",
    "    # Update estimated human traits\n",
    "    manufacturer_agent.update_estimated_human_traits(retailer_action)\n",
    "\n",
    "    # Update Q-table for manufacturer agent\n",
    "    manufacturer_agent.update_q_table(state, manufacturer_action, manufacturer_reward, next_state)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Retailer profit: {retailer_reward}, Manufacturer profit: {manufacturer_reward}\")\n",
    "\n",
    "    # Move to the next state\n",
    "    state = next_state\n",
    "\n",
    "print(\"Game over.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bu errorlu olan, ama doğru\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from gym import Env, spaces\n",
    "import random\n",
    "\n",
    "# Define the SupplyChainEnv class\n",
    "class SupplyChainEnv(Env):\n",
    "    \"\"\"\n",
    "    Custom Environment for the supply chain game.\n",
    "    \"\"\"\n",
    "    def __init__(self, contract_type=\"wholesale\"):\n",
    "        super(SupplyChainEnv, self).__init__()\n",
    "        self.contract_type = contract_type\n",
    "        self.max_stock = 150\n",
    "        self.max_price = 12\n",
    "        self.max_rounds = 40\n",
    "        self.current_round = 0\n",
    "\n",
    "        if self.contract_type == \"wholesale\":\n",
    "            self.manufacturer_action_space = spaces.Discrete(self.max_price + 1)\n",
    "            self.retailer_action_space = spaces.Discrete(self.max_stock + 1)\n",
    "        elif self.contract_type == \"buyback\":\n",
    "            self.manufacturer_action_space = spaces.MultiDiscrete([self.max_price + 1, self.max_price + 1])\n",
    "            self.retailer_action_space = spaces.Discrete(self.max_stock + 1)\n",
    "        elif self.contract_type == \"revenue-sharing\":\n",
    "            self.manufacturer_action_space = spaces.MultiDiscrete([self.max_price + 1, self.max_price + 1])\n",
    "            self.retailer_action_space = spaces.Discrete(self.max_stock + 1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid contract type.\")\n",
    "\n",
    "        # The state does not include the demand since it's unknown before decisions are made\n",
    "        self.observation_space = spaces.Box(low=0, high=150, shape=(4,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # At reset, we do not generate demand or include it in the state\n",
    "        self.state = np.array([0, 0, 0, 0])  # State: [Wholesale Price, Buyback Price, Revenue Share, Previous Profit]\n",
    "        self.current_round = 0\n",
    "        return self.state\n",
    "\n",
    "    def step(self, actions):\n",
    "        manufacturer_action, retailer_action = actions\n",
    "\n",
    "        # Unpack actions\n",
    "        if self.contract_type == \"wholesale\":\n",
    "            w = manufacturer_action  # Wholesale price\n",
    "            b = 0\n",
    "            r = 0\n",
    "        elif self.contract_type == \"buyback\":\n",
    "            w, b = manufacturer_action  # Wholesale price and Buyback price\n",
    "            r = 0\n",
    "        elif self.contract_type == \"revenue-sharing\":\n",
    "            w, r = manufacturer_action  # Wholesale price and Revenue share\n",
    "            b = 0\n",
    "\n",
    "        Q = retailer_action  # Retailer's order quantity\n",
    "\n",
    "        # Generate demand AFTER decisions are made\n",
    "        self.demand = np.random.randint(50, 151)\n",
    "\n",
    "        # Calculate sales and leftovers\n",
    "        sales = min(Q, self.demand)\n",
    "        leftovers = Q - sales\n",
    "        c = 3  # Manufacturer's production cost\n",
    "        p = 12  # Retail price\n",
    "\n",
    "        if self.contract_type == \"wholesale\":\n",
    "            # Retailer payoff\n",
    "            retailer_profit = p * sales - w * Q\n",
    "\n",
    "            # Manufacturer payoff\n",
    "            manufacturer_profit = (w - c) * Q\n",
    "\n",
    "        elif self.contract_type == \"buyback\":\n",
    "            # Enforce constraint: buyback price must not exceed wholesale price\n",
    "            if b > w:\n",
    "                b = w  # Adjust buyback price to wholesale price\n",
    "\n",
    "            # Retailer payoff\n",
    "            retailer_profit = p * sales - w * Q + b * leftovers\n",
    "\n",
    "            # Manufacturer payoff\n",
    "            manufacturer_profit = (w - c) * Q - b * leftovers\n",
    "\n",
    "        elif self.contract_type == \"revenue-sharing\":\n",
    "            # Enforce constraint: revenue share must not exceed (retail price - wholesale price)\n",
    "            max_revenue_share = p - w\n",
    "            if r > max_revenue_share:\n",
    "                r = max_revenue_share\n",
    "\n",
    "            # Retailer payoff\n",
    "            retailer_profit = (p - r) * sales - w * Q\n",
    "\n",
    "            # Manufacturer payoff\n",
    "            manufacturer_profit = (w - c) * Q + r * sales\n",
    "\n",
    "        # Update the state\n",
    "        # State now includes previous profits as feedback\n",
    "        self.state = np.array([w, b, r, retailer_profit])\n",
    "\n",
    "        self.current_round += 1\n",
    "        done = self.current_round >= self.max_rounds\n",
    "\n",
    "        # For simplicity, next state is the current state (excluding demand)\n",
    "        next_state = self.state.copy()\n",
    "\n",
    "        # Return next_state, rewards, done, info\n",
    "        return next_state, (manufacturer_profit, retailer_profit), done, {}\n",
    "\n",
    "# The rest of the code remains largely the same, but we need to adjust how states are represented\n",
    "# Update the QLearningAgent class if necessary\n",
    "\n",
    "# Import itertools for possible actions\n",
    "import itertools\n",
    "\n",
    "# Define the QLearningAgent class with Trait-Driven Reward Modulation and dynamic Exploration vs. Exploitation Balance\n",
    "class QLearningAgent:\n",
    "    def __init__(self, action_space, personality_traits=None, learning_rate=0.1, discount_factor=0.99,\n",
    "                 epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):\n",
    "        self.action_space = action_space\n",
    "        self.personality_traits = personality_traits or {}\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.initial_epsilon = epsilon  # Store initial epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.q_table = defaultdict(float)\n",
    "        self.estimated_human_traits = {'Risk Averse Coefficient': 0.5}\n",
    "        self.human_actions = []\n",
    "        self.behavior_change_threshold = 2  # Adjust as needed\n",
    "\n",
    "    def _state_to_key(self, state):\n",
    "        trait_values = tuple(self.personality_traits.values())\n",
    "        return tuple(state) + trait_values\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if self.detect_behavior_change():\n",
    "            self.epsilon = min(1.0, self.epsilon * 1.1)  # Increase epsilon to promote exploration\n",
    "        else:\n",
    "            self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)  # Decay epsilon\n",
    "\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Explore: choose a random action\n",
    "            if isinstance(self.action_space, spaces.Discrete):\n",
    "                return self.action_space.sample()\n",
    "            elif isinstance(self.action_space, spaces.MultiDiscrete):\n",
    "                return tuple(self.action_space.sample())\n",
    "        else:\n",
    "            # Exploit: choose the action with the highest Q-value\n",
    "            state_key = self._state_to_key(state)\n",
    "            possible_actions = self.get_possible_actions()\n",
    "            q_values = [self.q_table[(state_key, a)] for a in possible_actions]\n",
    "            max_q = max(q_values)\n",
    "            max_actions = [a for a, q in zip(possible_actions, q_values) if q == max_q]\n",
    "            chosen_action = random.choice(max_actions)\n",
    "            return chosen_action\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state):\n",
    "        state_key = self._state_to_key(state)\n",
    "        next_state_key = self._state_to_key(next_state)\n",
    "\n",
    "        # Get possible actions for next state\n",
    "        possible_actions = self.get_possible_actions()\n",
    "        max_next_q = max([self.q_table[(next_state_key, a)] for a in possible_actions])\n",
    "\n",
    "        # Adjust the reward based on estimated human traits\n",
    "        adjusted_reward = self.adjust_reward_based_on_traits(reward)\n",
    "\n",
    "        # Update Q-value using the adjusted reward\n",
    "        self.q_table[(state_key, action)] += self.learning_rate * (\n",
    "            adjusted_reward + self.discount_factor * max_next_q - self.q_table[(state_key, action)]\n",
    "        )\n",
    "\n",
    "    def update_estimated_human_traits(self, human_action):\n",
    "        self.human_actions.append(human_action)\n",
    "        if len(self.human_actions) > 10:\n",
    "            recent_actions = self.human_actions[-10:]\n",
    "            action_variance = np.var(recent_actions)\n",
    "            max_variance = (self.action_space.n - 1) ** 2 / 12  # Variance of uniform distribution\n",
    "            self.estimated_human_traits['Risk Averse Coefficient'] = max(0, min(1, 1 - action_variance / max_variance))\n",
    "\n",
    "    def adjust_reward_based_on_traits(self, reward):\n",
    "        risk_aversion = self.estimated_human_traits.get('Risk Averse Coefficient', 0.5)\n",
    "        trait_factor = 1.0 + (0.5 - risk_aversion)  # Adjust factor between 0.5 and 1.5\n",
    "        adjusted_reward = reward * trait_factor\n",
    "        return adjusted_reward\n",
    "\n",
    "    def detect_behavior_change(self):\n",
    "        if len(self.human_actions) < 20:\n",
    "            return False\n",
    "        recent_actions = self.human_actions[-10:]\n",
    "        previous_actions = self.human_actions[-20:-10]\n",
    "        recent_mean = np.mean(recent_actions)\n",
    "        previous_mean = np.mean(previous_actions)\n",
    "        if abs(recent_mean - previous_mean) > self.behavior_change_threshold:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_possible_actions(self):\n",
    "        if isinstance(self.action_space, spaces.Discrete):\n",
    "            return range(self.action_space.n)\n",
    "        elif isinstance(self.action_space, spaces.MultiDiscrete):\n",
    "            ranges = [range(n) for n in self.action_space.nvec]\n",
    "            possible_actions = list(itertools.product(*ranges))\n",
    "            return possible_actions\n",
    "\n",
    "# Function to extract traits for all manufacturers and retailers from the behavioral data\n",
    "def extract_traits(behavioral_data):\n",
    "    # Extract manufacturer traits\n",
    "    manufacturer_traits = behavioral_data[[\n",
    "        'Manufacturer_Self Esteem Average',\n",
    "        'Manufacturer_Regret Scale Average',\n",
    "        'Manufacturer_Risk Averse Coefficient',\n",
    "        'Manufacturer_Fairness Index'\n",
    "    ]].rename(columns=lambda col: col.replace('Manufacturer_', '')).to_dict(orient='index')\n",
    "\n",
    "    # Extract retailer traits\n",
    "    retailer_traits = behavioral_data[[\n",
    "        'Retailer_Self Esteem Average',\n",
    "        'Retailer_Regret Scale Average',\n",
    "        'Retailer_Risk Averse Coefficient',\n",
    "        'Retailer_Fairness Index'\n",
    "    ]].rename(columns=lambda col: col.replace('Retailer_', '')).to_dict(orient='index')\n",
    "\n",
    "    return manufacturer_traits, retailer_traits\n",
    "\n",
    "# Load behavioral data\n",
    "behavioral_data = pd.read_csv('../adjusted_reponse_survey.csv')\n",
    "behavioral_data.columns = behavioral_data.columns.str.strip()\n",
    "\n",
    "# Extract traits\n",
    "manufacturer_traits_dict, retailer_traits_dict = extract_traits(behavioral_data.set_index('PLAYER NAME'))\n",
    "\n",
    "# For simplicity, select a sample manufacturer and retailer\n",
    "sample_manufacturer = list(manufacturer_traits_dict.keys())[0]\n",
    "sample_retailer = list(retailer_traits_dict.keys())[0]\n",
    "\n",
    "sample_manufacturer_traits = manufacturer_traits_dict[sample_manufacturer]\n",
    "sample_retailer_traits = retailer_traits_dict[sample_retailer]\n",
    "\n",
    "# Load and preprocess historical data\n",
    "def preprocess_historical_data(df, contract_type):\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Ensure that we get scalar values by converting to float\n",
    "        wholesale_price = float(row['Wholesale_p.'])\n",
    "        buyback_price = float(row['Buyback_p.']) if 'Buyback_p.' in row and not pd.isnull(row['Buyback_p.']) else 0.0\n",
    "        revenue_share = float(row['Revenue_Share']) if 'Revenue_Share' in row and not pd.isnull(row['Revenue_Share']) else 0.0\n",
    "        previous_retailer_profit = float(row['Realized_Retailer_Profit'])\n",
    "        stock = int(row['Stock'])\n",
    "        realized_mfg_profit = float(row['Realized_Mfg_Profit'])\n",
    "        realized_retailer_profit = float(row['Realized_Retailer_Profit'])\n",
    "\n",
    "        # State variables\n",
    "        state = [\n",
    "            wholesale_price,\n",
    "            buyback_price,\n",
    "            revenue_share,\n",
    "            previous_retailer_profit\n",
    "        ]\n",
    "\n",
    "        # Actions\n",
    "        if contract_type == \"wholesale\":\n",
    "            manufacturer_action = wholesale_price\n",
    "        else:\n",
    "            # For 'buyback' and 'revenue-sharing' contracts\n",
    "            if contract_type == \"buyback\":\n",
    "                manufacturer_action = (wholesale_price, buyback_price)\n",
    "            elif contract_type == \"revenue-sharing\":\n",
    "                manufacturer_action = (wholesale_price, revenue_share)\n",
    "            else:\n",
    "                manufacturer_action = (wholesale_price, 0.0)  # Default\n",
    "\n",
    "        retailer_action = stock\n",
    "\n",
    "        # Rewards\n",
    "        manufacturer_reward = realized_mfg_profit\n",
    "        retailer_reward = realized_retailer_profit\n",
    "\n",
    "        # Next state (use next period's data if available, else same state)\n",
    "        next_state = state.copy()\n",
    "\n",
    "        data.append((state, manufacturer_action, retailer_action, (manufacturer_reward, retailer_reward), next_state))\n",
    "    return data\n",
    "\n",
    "print(\"Wholesale Data Columns:\", historical_data_wholesale.columns)\n",
    "print(\"Buyback Data Columns:\", historical_data_buyback.columns)\n",
    "print(\"Revenue Sharing Data Columns:\", historical_data_revenue_sharing.columns)\n",
    "\n",
    "# Load historical data\n",
    "historical_data_wholesale = pd.read_csv('../Notebooks/experiment/curr_data.csv')\n",
    "historical_data_buyback = pd.read_csv('../Notebooks/experiment/curr_data_bb.csv')\n",
    "historical_data_revenue_sharing = pd.read_csv('../Notebooks/experiment/curr_data_rs.csv')\n",
    "\n",
    "# Clean column names\n",
    "historical_data_wholesale.columns = historical_data_wholesale.columns.str.strip().str.replace(' ', '_')\n",
    "historical_data_buyback.columns = historical_data_buyback.columns.str.strip().str.replace(' ', '_')\n",
    "historical_data_revenue_sharing.columns = historical_data_revenue_sharing.columns.str.strip().str.replace(' ', '_')\n",
    "\n",
    "# Process historical data\n",
    "historical_data_wholesale = preprocess_historical_data(historical_data_wholesale, \"wholesale\")\n",
    "historical_data_buyback = preprocess_historical_data(historical_data_buyback, \"buyback\")\n",
    "historical_data_revenue_sharing = preprocess_historical_data(historical_data_revenue_sharing, \"revenue-sharing\")\n",
    "\n",
    "# Combine historical data\n",
    "historical_data = historical_data_wholesale + historical_data_buyback + historical_data_revenue_sharing\n",
    "\n",
    "# Initialize the environment\n",
    "env = SupplyChainEnv(contract_type=\"wholesale\")  # You can change the contract type as needed\n",
    "\n",
    "# Initialize agents with personality traits\n",
    "manufacturer_agent = QLearningAgent(env.manufacturer_action_space, personality_traits=sample_manufacturer_traits)\n",
    "retailer_agent = QLearningAgent(env.retailer_action_space, personality_traits=sample_retailer_traits)\n",
    "\n",
    "# Pre-train the agents using historical data\n",
    "for record in historical_data:\n",
    "    state, manufacturer_action, retailer_action, rewards, next_state = record\n",
    "    manufacturer_reward, retailer_reward = rewards\n",
    "\n",
    "    # Update Q-table for manufacturer\n",
    "    state_key = manufacturer_agent._state_to_key(state)\n",
    "    manufacturer_agent.q_table[(state_key, manufacturer_action)] = manufacturer_reward\n",
    "\n",
    "    # Update Q-table for retailer\n",
    "    state_key = retailer_agent._state_to_key(state)\n",
    "    retailer_agent.q_table[(state_key, retailer_action)] = retailer_reward\n",
    "\n",
    "# Main training loop\n",
    "num_episodes = 100  # Number of episodes for training\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Agents select actions\n",
    "        manufacturer_action = manufacturer_agent.get_action(state)\n",
    "        retailer_action = retailer_agent.get_action(state)\n",
    "\n",
    "        # Environment step\n",
    "        next_state, rewards, done, info = env.step((manufacturer_action, retailer_action))\n",
    "        manufacturer_reward, retailer_reward = rewards\n",
    "\n",
    "        # Update estimated human traits (assuming retailer is human for this example)\n",
    "        # In this example, both agents are AI, so we can simulate human actions\n",
    "        # For real human interaction, you would get human_action from user input\n",
    "        manufacturer_agent.update_estimated_human_traits(retailer_action)\n",
    "        retailer_agent.update_estimated_human_traits(manufacturer_action)\n",
    "\n",
    "        # Update Q-tables\n",
    "        manufacturer_agent.update_q_table(state, manufacturer_action, manufacturer_reward, next_state)\n",
    "        retailer_agent.update_q_table(state, retailer_action, retailer_reward, next_state)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "    # Optionally, print progress\n",
    "    print(f\"Episode {episode + 1}/{num_episodes} completed.\")\n",
    "\n",
    "# The agents are now trained and can be used to play the game against a human\n",
    "# For example, to have the AI manufacturer play against a human retailer:\n",
    "\n",
    "# Initialize environment\n",
    "env = SupplyChainEnv(contract_type=\"wholesale\")  # Adjust contract type as needed\n",
    "\n",
    "# Reset environment\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "print(\"Game start. You are the Retailer.\")\n",
    "\n",
    "while not done:\n",
    "    # Manufacturer (AI) selects action\n",
    "    manufacturer_action = manufacturer_agent.get_action(state)\n",
    "\n",
    "    # Human (Retailer) input\n",
    "    print(f\"Manufacturer offers wholesale price: {manufacturer_action}\")\n",
    "    retailer_action = int(input(\"Enter your order quantity (0-150): \"))\n",
    "\n",
    "    # Ensure the action is within the valid range\n",
    "    retailer_action = max(0, min(env.retailer_action_space.n - 1, retailer_action))\n",
    "\n",
    "    # Environment step\n",
    "    next_state, rewards, done, info = env.step((manufacturer_action, retailer_action))\n",
    "    manufacturer_reward, retailer_reward = rewards\n",
    "\n",
    "    # Update estimated human traits\n",
    "    manufacturer_agent.update_estimated_human_traits(retailer_action)\n",
    "\n",
    "    # Update Q-table for manufacturer agent\n",
    "    manufacturer_agent.update_q_table(state, manufacturer_action, manufacturer_reward, next_state)\n",
    "\n",
    "    # Display results\n",
    "    print(f\"Demand was: {env.demand}\")\n",
    "    print(f\"Your profit: {retailer_reward}, Manufacturer profit: {manufacturer_reward}\")\n",
    "\n",
    "    # Move to the next state\n",
    "    state = next_state\n",
    "\n",
    "print(\"Game over.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
